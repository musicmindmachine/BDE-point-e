{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Future :)\n",
    "Notebook by Rayce Stipanovich\n",
    "\n",
    "This is a Python Jupyter Notebook designed to output low-poly .gltf files for use in 3D content or web stuff.\n",
    "\n",
    "### Relies on:\n",
    "- Python v3, Jupyter Notebooks\n",
    "- NVIDIA CUDA v11.7 (or your CPU if you're a noob)\n",
    "- OpenAI's Point-E Beta\n",
    "- Blender w/ .gltf Exporter extension\n",
    "\n",
    "### Installation and First-Time-Setup\n",
    "1. Make sure you have [Python 3 installed.](https://www.python.org/downloads/)\n",
    "2. Install the the [NVIDIA CUDA drivers v11.7](https://developer.nvidia.com/cuda-11-7-0-download-archive)\n",
    "3. Install the [Python VSCode Extension](https://marketplace.visualstudio.com/items?itemName=ms-python.python)\n",
    "4. Install the [Jupyter VSCode Extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter)\n",
    "5. Install [Blender](https://www.blender.org/download/) if you want GLTF Conversion\n",
    "6. Follow the rest of the instructions below...\n",
    "\n",
    "___\n",
    "\n",
    "## First-Time Setup (Continued)\n",
    "Run the pip installer to install the [CUDA 11.7 version of PyTorch](https://pytorch.org/get-started/locally/) along with the other dependanceis Point-E Needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/Rayce/point_e/point-e\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting clip@ git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\rayce\\appdata\\local\\temp\\pip-install-mh0dxtw4\\clip_1d9b786a494f40768a15463b68d4f6a5\n",
      "  Resolved https://github.com/openai/CLIP.git to commit d50d76daa670286dd6cacf3bcd80b5e4823fc8e1\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: filelock in c:\\users\\rayce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from point-e==0.0.0) (3.9.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\rayce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from point-e==0.0.0) (9.3.0)\n",
      "Requirement already satisfied: torch in c:\\users\\rayce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from point-e==0.0.0) (1.13.1+cu117)\n",
      "Requirement already satisfied: fire in c:\\users\\rayce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from point-e==0.0.0) (0.5.0)\n",
      "Requirement already satisfied: humanize in c:\\users\\rayce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from point-e==0.0.0) (4.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\rayce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from point-e==0.0.0) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rayce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from point-e==0.0.0) (4.64.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\rayce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from point-e==0.0.0) (3.6.2)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\rayce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from point-e==0.0.0) (0.19.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\rayce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from point-e==0.0.0) (1.9.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\rayce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from point-e==0.0.0) (1.24.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\rayce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from point-e==0.0.0) (5.11.0)\n",
      "Collecting IProgress\n",
      "  Using cached IProgress-0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\rayce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from point-e==0.0.0) (8.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\Rayce\\AppData\\Local\\Temp\\pip-install-mh0dxtw4\\clip_1d9b786a494f40768a15463b68d4f6a5'\n",
      "ERROR: Could not find a version that satisfies the requirement juptyer (from point-e) (from versions: none)\n",
      "ERROR: No matching distribution found for juptyer\n"
     ]
    }
   ],
   "source": [
    "# install dependencies with pip install\n",
    "%pip install -e ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can skip this next step if you don't have an NVIDIA GPU.  This will make sure we have the correct version of PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pytorch w/ cuda 11.7\n",
    "%pip uninstall torch -y\n",
    "%pip install torch --force-reinstall --extra-index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STOP!  \n",
    "You may need to \"restart\" the kernel if you're having errors past this point.  There should be a Restart button towards the top center of the notebook.  Restarting can also undo what we just installed, so try and see what works if one way or the other doesn't for you.\n",
    "\n",
    "___\n",
    "\n",
    "# START HERE EVERY TIME YOU LAUNCH\n",
    "Next, run these chunks to import the dependances we just installed into our script we're building...  This loads up PyTorch and the Point-E Code for us.\n",
    "\n",
    "If you get an error saying `\"IProgress not found. Please update jupyter and ipywidgets.\"`, just ignore it for now.  Restarting may fix, but it also might mean you need to run those two `%pip install` steps again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: PyTorch Version: 1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch and Point-E Libs...\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"vscode\"\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from point_e.diffusion.configs import DIFFUSION_CONFIGS, diffusion_from_config\n",
    "from point_e.diffusion.sampler import PointCloudSampler\n",
    "from point_e.models.download import load_checkpoint\n",
    "from point_e.models.configs import MODEL_CONFIGS, model_from_config\n",
    "from point_e.util.pc_to_mesh import marching_cubes_mesh\n",
    "from point_e.util.plotting import plot_point_cloud\n",
    "from point_e.util.point_cloud import PointCloud\n",
    "print(\"OK: PyTorch Version:\", torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOP\n",
    "Sanity check that CUDA is working on the GPU.... otherwise this will take forever...\n",
    "If you see CPU, then HMU to help debug.  You should see CUDA here!\n",
    "\n",
    "Something Like...\n",
    "\n",
    "    Using NVIDIA CUDA with PyTorch: v1.13.1+cu117 and CUDA: v11.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA CUDA with PyTorch: v1.13.1+cu117 and CUDA: v11.7\n",
      "\n",
      "NVCC Version:\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Jun__8_16:59:34_Pacific_Daylight_Time_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.99\n",
      "Build cuda_11.7.r11.7/compiler.31442593_0\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "   print('Using NVIDIA CUDA with PyTorch: v'+torch.__version__+' and CUDA: v'+torch.version.cuda)\n",
    "else:\n",
    "   print('Using CPU with PyTorch: v'+torch.__version__)\n",
    "print(\"\\nNVCC Version:\")\n",
    "!nvcc --version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### One-Time-Setup after Launch and CUDA is OK.\n",
    "Import the ML Models into the GPU and initialize the point cloud.  Run these once after you get torch with CUDA up and running.\n",
    "\n",
    "Here is where we also say what version of Blender we would like to use to convet the .ply model intermediates into .gltf files.\n",
    "\n",
    "Run this code if you want to generate or convert meshes with blender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: Blender Path: C:\\Program Files\\Blender Foundation\\Blender 3.0\\blender.exe\n",
      "Using Blender Version: 3.0\n"
     ]
    }
   ],
   "source": [
    "# Setup Blender Converter\n",
    "blender_version = 3.0\n",
    "blender_path = \"C:\\Program Files\\Blender Foundation\\Blender \"+str(blender_version)+\"\\\\blender.exe\"\n",
    "\n",
    "# Check if Blender is installed\n",
    "with open(blender_path, \"r\"):\n",
    "    print(\"OK: Blender Path:\", blender_path)\n",
    "print(\"Using Blender Version:\", blender_version)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the ML model by downloading the models onto the GPU and pulling the checkpoint training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating base model...\n",
      "creating upsample model...\n",
      "creating Signed Density Field model...\n",
      "downloading base checkpoint...\n",
      "downloading upsampler checkpoint...\n",
      "downloading SDF checkpoint...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unpack Preapre the ML models on the GPU or CPU...\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('creating base model...')\n",
    "base_name = 'base40M-textvec'\n",
    "base_model = model_from_config(MODEL_CONFIGS[base_name], device)\n",
    "base_model.eval()\n",
    "base_diffusion = diffusion_from_config(DIFFUSION_CONFIGS[base_name])\n",
    "\n",
    "print('creating upsample model...')\n",
    "upsampler_model = model_from_config(MODEL_CONFIGS['upsample'], device)\n",
    "upsampler_model.eval()\n",
    "upsampler_diffusion = diffusion_from_config(DIFFUSION_CONFIGS['upsample'])\n",
    "\n",
    "print('creating Signed Density Field model...')\n",
    "sdf_model = model_from_config(MODEL_CONFIGS['sdf'], device)\n",
    "sdf_model.eval()\n",
    "\n",
    "print('downloading base checkpoint...')\n",
    "base_model.load_state_dict(load_checkpoint(base_name, device))\n",
    "\n",
    "print('downloading upsampler checkpoint...')\n",
    "upsampler_model.load_state_dict(load_checkpoint('upsample', device))\n",
    "\n",
    "print('downloading SDF checkpoint...')\n",
    "sdf_model.load_state_dict(load_checkpoint('sdf', device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Some interesting config stuff here.  This is where you can reaaally mess with the `Point Cloud Sampler's` interative steps and whatnot.  \n",
    "Leave as defaults otherwise :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point Cloud Sampler Initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize the point cloud sampler\n",
    "num_points = 4096\n",
    "point_chunks = 1024\n",
    "denoised = True,\n",
    "guidance_scale = [3.0, 0.0],\n",
    "use_karras = (True, True),\n",
    "karras_steps = (64, 64),\n",
    "sigma_min = (1e-3, 1e-3),\n",
    "sigma_max = (120, 160),\n",
    "s_churn = (3, 0),\n",
    "\n",
    "sampler = PointCloudSampler(\n",
    "    device=device,\n",
    "    # clip_denoised=denoised,\n",
    "    # use_karras=use_karras,\n",
    "    # karras_steps=karras_steps,\n",
    "    # sigma_min=sigma_min,\n",
    "    # sigma_max=sigma_max,\n",
    "    # s_churn=s_churn,\n",
    "    guidance_scale=guidance_scale,\n",
    "    models=[base_model, upsampler_model],\n",
    "    diffusions=[base_diffusion, upsampler_diffusion],\n",
    "    num_points=[point_chunks, num_points - point_chunks],\n",
    "    aux_channels=['R', 'G', 'B'],\n",
    "    model_kwargs_key_filter=('texts', ''), # Do not condition the upsampler at all\n",
    ")\n",
    "print(\"Point Cloud Sampler Initialized\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### THE FUN PART!!!\n",
    "Keep Re-Running this code afterwards to generate new models based on your prompt.  Remember to re-run the prompt chunk below toa ctually update the prompt on the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0687c0cbf3948bc822f3d88f528863a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39m# Produce a sample from the model.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m samples \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tqdm(sampler\u001b[39m.\u001b[39msample_batch_progressive(batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, model_kwargs\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(texts\u001b[39m=\u001b[39m[prompt]))):\n\u001b[0;32m      8\u001b[0m     samples \u001b[39m=\u001b[39m x\n\u001b[0;32m     10\u001b[0m \u001b[39m# Generate the point cloud and plot it.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Rayce\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\notebook.py:259\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[1;32m--> 259\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[0;32m    260\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    261\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m    262\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Rayce\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Rayce\\point_e\\point-e\\point_e\\diffusion\\sampler.py:163\u001b[0m, in \u001b[0;36mPointCloudSampler.sample_batch_progressive\u001b[1;34m(self, batch_size, model_kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m         internal_batch_size \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m    156\u001b[0m     samples_it \u001b[39m=\u001b[39m diffusion\u001b[39m.\u001b[39mp_sample_loop_progressive(\n\u001b[0;32m    157\u001b[0m         model,\n\u001b[0;32m    158\u001b[0m         shape\u001b[39m=\u001b[39m(internal_batch_size, \u001b[39m*\u001b[39msample_shape[\u001b[39m1\u001b[39m:]),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m         clip_denoised\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_denoised,\n\u001b[0;32m    162\u001b[0m     )\n\u001b[1;32m--> 163\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m samples_it:\n\u001b[0;32m    164\u001b[0m     samples \u001b[39m=\u001b[39m x[\u001b[39m\"\u001b[39m\u001b[39mpred_xstart\u001b[39m\u001b[39m\"\u001b[39m][:batch_size]\n\u001b[0;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlow_res\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m stage_model_kwargs:\n",
      "File \u001b[1;32mc:\\Users\\Rayce\\point_e\\point-e\\point_e\\diffusion\\k_diffusion.py:181\u001b[0m, in \u001b[0;36mkarras_sample_progressive\u001b[1;34m(diffusion, model, shape, steps, clip_denoised, progress, model_kwargs, device, sigma_min, sigma_max, rho, sampler, s_churn, s_tmin, s_tmax, s_noise, guidance_scale)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     guided_denoiser \u001b[39m=\u001b[39m denoiser\n\u001b[1;32m--> 181\u001b[0m \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m sample_fn(\n\u001b[0;32m    182\u001b[0m     guided_denoiser,\n\u001b[0;32m    183\u001b[0m     x_T,\n\u001b[0;32m    184\u001b[0m     sigmas,\n\u001b[0;32m    185\u001b[0m     progress\u001b[39m=\u001b[39mprogress,\n\u001b[0;32m    186\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msampler_args,\n\u001b[0;32m    187\u001b[0m ):\n\u001b[0;32m    188\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(diffusion, GaussianDiffusion):\n\u001b[0;32m    189\u001b[0m         \u001b[39myield\u001b[39;00m diffusion\u001b[39m.\u001b[39munscale_out_dict(obj)\n",
      "File \u001b[1;32mc:\\Users\\Rayce\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\grad_mode.py:43\u001b[0m, in \u001b[0;36m_DecoratorContextManager._wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m     \u001b[39m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 43\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m     45\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m             \u001b[39m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Rayce\\point_e\\point-e\\point_e\\diffusion\\k_diffusion.py:265\u001b[0m, in \u001b[0;36msample_heun\u001b[1;34m(denoiser, x, sigmas, progress, s_churn, s_tmin, s_tmax, s_noise)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39mif\u001b[39;00m gamma \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    264\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m eps \u001b[39m*\u001b[39m (sigma_hat\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m-\u001b[39m sigmas[i] \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[1;32m--> 265\u001b[0m denoised \u001b[39m=\u001b[39m denoiser(x, sigma_hat \u001b[39m*\u001b[39;49m s_in)\n\u001b[0;32m    266\u001b[0m d \u001b[39m=\u001b[39m to_d(x, sigma_hat, denoised)\n\u001b[0;32m    267\u001b[0m \u001b[39myield\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m: x, \u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m: i, \u001b[39m\"\u001b[39m\u001b[39msigma\u001b[39m\u001b[39m\"\u001b[39m: sigmas[i], \u001b[39m\"\u001b[39m\u001b[39msigma_hat\u001b[39m\u001b[39m\"\u001b[39m: sigma_hat, \u001b[39m\"\u001b[39m\u001b[39mpred_xstart\u001b[39m\u001b[39m\"\u001b[39m: denoised}\n",
      "File \u001b[1;32mc:\\Users\\Rayce\\point_e\\point-e\\point_e\\diffusion\\k_diffusion.py:175\u001b[0m, in \u001b[0;36mkarras_sample_progressive.<locals>.guided_denoiser\u001b[1;34m(x_t, sigma)\u001b[0m\n\u001b[0;32m    173\u001b[0m x_0 \u001b[39m=\u001b[39m denoiser(x_t, sigma)\n\u001b[0;32m    174\u001b[0m cond_x_0, uncond_x_0 \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39msplit(x_0, \u001b[39mlen\u001b[39m(x_0) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m--> 175\u001b[0m x_0 \u001b[39m=\u001b[39m uncond_x_0 \u001b[39m+\u001b[39m guidance_scale \u001b[39m*\u001b[39;49m (cond_x_0 \u001b[39m-\u001b[39;49m uncond_x_0)\n\u001b[0;32m    176\u001b[0m \u001b[39mreturn\u001b[39;00m x_0\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "# Set a prompt to condition on.\n",
    "prompt = 'apple tree'\n",
    "previewGridSize = 3 # 3x3 grid of previews\n",
    "\n",
    "# Produce a sample from the model.\n",
    "samples = None\n",
    "for x in tqdm(sampler.sample_batch_progressive(batch_size=1, model_kwargs=dict(texts=[prompt]))):\n",
    "    samples = x\n",
    "    \n",
    "# Generate the point cloud and plot it.\n",
    "pc = sampler.output_to_point_clouds(samples)[0]\n",
    "fig = plot_point_cloud(pc, grid_size=previewGridSize, fixed_bounds=((-0.75, -0.75, -0.75),(0.75, 0.75, 0.75)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to look at the 3D Point Cloud Data ina browser!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a plotly figure\n",
    "fig_plotly = go.Figure(\n",
    "   data=[\n",
    "      go.Scatter3d(\n",
    "         x=pc.coords[:, 0],y=pc.coords[:, 1],z=pc.coords[:, 2],\n",
    "         mode='markers',\n",
    "         marker=dict(\n",
    "            size=5,\n",
    "            color=['rgb({},{},{})'.format(r,g,b) for r,g,b in zip(pc.channels['R'], pc.channels['G'], pc.channels['B'])],\n",
    "         )\n",
    "      )\n",
    "   ],\n",
    "   layout=dict(\n",
    "      scene=dict(\n",
    "         bgcolor=\"rgb(30, 30, 30)\",\n",
    "         xaxis=dict(visible=False),\n",
    "         yaxis=dict(visible=False),\n",
    "         zaxis=dict(visible=False),\n",
    "      )\n",
    "   ),  \n",
    ")\n",
    "\n",
    "# Render the plotly figure\n",
    "fig_plotly.show(renderer=\"browser\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "This code outputs the raw point cloud data, unmeshed, into the `raw_points` folder as .ply files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the mesh to a PLY file to import into some other program.\n",
    "newP = prompt.replace(\" \", \"_\") + \"_\" + str(random.randint(1, 1000000))\n",
    "rawName = 'raw_points/'+newP+\".raw.ply\"\n",
    "\n",
    "with open(rawName, 'wb') as f:\n",
    "    pc.write_ply(f)\n",
    "    \n",
    "print(\"Generated point cloud saved to: \"+rawName)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "This next code should mesh-ify the point cloud data into triangles for rendering.  This gets exported to the intermediates folder, but these are still.ply files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a mesh (with vertex colors) from the point cloud. and write it to a PLY file.\n",
    "mesh = marching_cubes_mesh(\n",
    "    pc=pc,\n",
    "    model=sdf_model,\n",
    "    batch_size=4096,\n",
    "    grid_size=128, # increase to 128 for resolution used in evals - 32 is faster\n",
    "    progress=True,\n",
    ")\n",
    "\n",
    "newP = prompt.replace(\" \", \"_\") + \"_\" + str(random.randint(1, 1000000))\n",
    "fileName = 'intermediates/'+newP+\".ply\"\n",
    "\n",
    "# Write the mesh to a PLY file to import into some other program.\n",
    "with open(fileName, 'wb') as f:\n",
    "    mesh.write_ply(f)\n",
    "    \n",
    "print(\"Generated mesh saved to: \"+fileName)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Run the code below to use Blender to convert the meshed .ply into a .gltf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the .ply file to embedded .gltf file using Blender\n",
    "!\"$blender_path\" -b -P 2gltf2.py -- \"$fileName\"\n",
    "outputName = fileName.replace(\".ply\",\".gltf\").replace(\"intermediates/\",\"outputs/\")\n",
    "print(\"\\n============================================\\n\\n   .gltf file exported successfully! :)\")\n",
    "print(\"\\n\\n at the path: \" + outputName)\n",
    "print(\"\\n\\n============================================\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!  You should now have a .ply or a .gltf model!\n",
    "\n",
    "You can preview them with the 3D viewer in windows, or at [3DViewer.net](https://3DViewer.net) on a mac.\n",
    "\n",
    "# BDE â™¥\n",
    "(Best Day Ever!!!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70299803923d8379d45d42bf2e8e5dbb07f8d62052670777b1d5248d0ffe4c08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
